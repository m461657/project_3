---
title: "How can we use the data science process to predict our favorite artists?" 
author: "Megan Willis & Kyle Goulart"
date: "May 9, 2020"
output: 
  html_document:
    theme: "spacelab"
    code_folding: hide
---

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(ggthemes)
library(lubridate)
library(rpart.plot)
library(ggthemr)
```

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Our dataset: Spotify Songs
spotify_songs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')
```

## Introduction

Our goal for this project is to go through the entire data science process, starting with some data exploration, data wrangling, data viz, all the way through creating a predictive model using `tidymodels`! We will be using the `spotify_songs` dataset from the *R for Data Science* github. Our final portion of the project will be predicting `track_artist` from a long list of features (specifically: track_popularity, playlist_genre, danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, tempo, release_year). 

We also set a theme for all our graphics using `ggthemr`, and saved the color **Spotify** uses for our graphics later on.
```{r, warning = FALSE, message = FALSE}
ggthemr("chalk", type = "outer", layout = "minimal")

spotify_green = rgb(30, 215, 96, max=255)
```

## Data Wrangling

### Filter for our favorite artists

We decided to filter the `spotify_songs` dataset and create a new dataset with some of our favorite artists, since the original dataset contained over 32,000 observations. We ended up choosing 14 of our favorite artists, each with varied total number of tracks, as well as many different genres. These artists include: 

- Tame Impala
- A$AP Rocky
- Fleetwood Mac
- Queen
- Childish Gambino
- Creedence Clearwater Revival
- Elton John
- Tyler, The Creator
- Michael Jackson
- Post Malone
- Logic
- Vampire Weekend
- Chance the Rapper
- Mac DeMarco

This code is used to filter for our 14 favorite artists, as well as select the columns in the original `spotify_songs` dataset we wanted to include in our new `favorite_artists` dataset.
```{r}
favorite_artists = spotify_songs %>% 
  filter(track_artist %in% c("Tame Impala", "A$AP Rocky", "Fleetwood Mac", "Queen", "Childish Gambino", "Creedence Clearwater Revival", "Elton John", "Tyler, The Creator", "Michael Jackson", "Post Malone", "Logic", "Vampire Weekend", "Chance the Rapper", "Mac DeMarco")) %>% 
  select(track_artist, track_name, track_album_name, track_album_release_date, track_popularity, playlist_genre, danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, tempo, duration_ms)
```

### Create a column for release year

This code is used to extract the year from the `track_album_release_date` column, save it as `release_year`, and then convert `release_year` to a numeric (before, it was a character string). Then, we reselcted the columns, and included our new column `release_year` instead of `track_album_release_date`.
```{r}
favorite_artists = favorite_artists %>% 
  mutate(release_year = substring(track_album_release_date, 1, 4)) %>% 
  mutate(release_year = as.numeric(release_year)) %>% 
  select(track_artist, track_name, track_album_name, release_year, track_popularity, playlist_genre, danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, tempo, duration_ms)
```

## Data Exploration

Before beginning the process of creating a predictive model, we wanted to explore the relationships between some of our features and our selected artists. Specifically, we wanted to see if there was variation from artist to artist among the selected features.

We wanted to explore the number of songs each artist we chose had, because we are concerned that the model may do a better job at predicting the artists with a higher song count. This graphic shows that **Queen** has the most songs in this dataset, followed by **Logic** and **Post Malone**.
```{r}
favorite_artists %>% 
  group_by(track_artist) %>% 
  count() %>% 
  ggplot(aes(x = fct_reorder(track_artist, n), y = n))+
  geom_col(fill = spotify_green, color = "gray40")+
  coord_flip()+
  labs(x = "", y = "Song Count", title = "Number of Songs per Artist", caption = "Source: Song Genres via R for Data Science")+
  theme(plot.caption = element_text(size = 8, hjust = -.75), plot.title = element_text(hjust = .5, size = 18))
```


For the next few graphics, we wanted to see the variations in some of the features for each artist, that we will be using in our predictive model.

First up, we calculated the average popularity of each artist using the `track_popularity` column. We then created a graphic displaying the order of popularity of the artists that we hand-picked earlier. We wanted to explore the variations in averge populartiy between each of the artist to maybe use in our model. This graphic shows that the average `track_popularity` **does** vary for each artist, with the highest mean popularity going to Tyler, The Creator.
```{r}
favorite_artists %>% 
  group_by(track_artist) %>% 
  summarize(mean_popularity = mean(track_popularity)) %>% 
  ggplot(aes(x = fct_reorder(track_artist, mean_popularity), y = mean_popularity))+
  geom_col(fill = spotify_green, color = "gray40")+
  coord_flip()+
  labs(x = "", y = "Average Track Popularity", title = "Average Track Popularity per Artist", caption = "Source: Song Genres via R for Data Science")+
  theme(plot.caption = element_text(size = 8, hjust = -.75), plot.title = element_text(hjust = .5, size = 18))
```


This block of code is for `danceability`. We calulated the average danceability of each artist based on their tracks and plotted the results from greatest to least. Like the graphic above, there is a clear variation in the average `danceability` from artist to artist, with Michael Jackson coming out on top (DUH!).
```{r}
favorite_artists %>% 
  group_by(track_artist) %>% 
  summarize(mean_danceability = mean(danceability)) %>% 
  ggplot(aes(x = fct_reorder(track_artist, mean_danceability), y = mean_danceability))+
  geom_col(fill = spotify_green, color = "gray40")+
  coord_flip()+
  labs(x = "", y = "Average Danceability", title = "Average Danceability by Artist", caption = "Source: Song Genres via R for Data Science")+
  theme(plot.caption = element_text(size = 8, hjust = -.75), plot.title = element_text(hjust = .5, size = 18))
```


Next, we decided to explore `tempo`. Again we calulated the average tempo for each artist based on their tracks and plotted them from greatest to least. Vapire weekend has the highest average tempo against all the other artists, again showing mass variations.
```{r}
favorite_artists %>% 
  group_by(track_artist) %>% 
  summarize(mean_tempo = mean(tempo)) %>% 
  ggplot(aes(x = fct_reorder(track_artist, mean_tempo), y = mean_tempo))+
  geom_col(fill = spotify_green, color = "gray40")+
  coord_flip()+
  labs(x = "", y = "Average Tempo", title = "Average Tempo by Artist", caption = "Source: Song Genres via R for Data Science")+
  theme(plot.caption = element_text(size = 8, hjust = -.75), plot.title = element_text(hjust = .5, size = 18))
```


Then, we moved on to `instrumentalness`. We calculated the average instrumentalness for each artist and the results reveal that Mac DeMarco had the highest instrumentalness among all of our 14 artists. This, of course, may be skewed due to Mac DeMarco only having 4 songs in the dataset.
```{r}
favorite_artists %>% 
  group_by(track_artist) %>% 
  summarize(mean_instrumentalness = mean(instrumentalness)) %>% 
  ggplot(aes(x = fct_reorder(track_artist, mean_instrumentalness), y = mean_instrumentalness))+
  geom_col(fill = spotify_green, color = "gray40")+
  coord_flip()+
  labs(x = "", y = "Average Instrumentalness", title = "Average Instrumentalness by Artist", caption = "Source: Song Genres via R for Data Science")+
  theme(plot.caption = element_text(size = 8, hjust = -.75), plot.title = element_text(hjust = .5, size = 18))
```


Finally, we explored the average `acousticness` for each of our 14 artists. Graphing it the same way as the others before, we find that Vampire Weekend again comes out on top, but this time for acousticness. This graph also shows deep variations between the artists based on the acousticness of their tracks.
```{r}
favorite_artists %>% 
  group_by(track_artist) %>% 
  summarize(mean_acousticness = mean(acousticness)) %>% 
  ggplot(aes(x = fct_reorder(track_artist, mean_acousticness), y = mean_acousticness))+
  geom_col(fill = spotify_green, color = "gray40")+
  coord_flip()+
  labs(x = "", y = "Average Acousticness", title = "Average Acousticness by Artist", caption = "Source: Song Genres via R for Data Science")+
  theme(plot.caption = element_text(size = 8, hjust = -.75), plot.title = element_text(hjust = .5, size = 18))
```


Each of these graphics give us a better understanding of these features and their relationship with our 14 favorite artists. This information supports our decision to use these features and more in our predictive model.

## Predictive Model

The final part of our project is to create a predicitve model using our data. To make things easier, we created a new dataset called `artists_model` which excludes some unnecessary columns from `favorite_artists`. Also, we converted the `track_artist` column from a character type to a factor because, for classification trees, the target needs to be a factor.
```{r, warning = FALSE, message = FALSE}
artists_model = favorite_artists %>% 
  select(track_artist, track_popularity, playlist_genre, danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, tempo, release_year)

artists_model$track_artist = as.factor(artists_model$track_artist)
```

In this block of code, we split the data with a 70/30 proportion and used this to create a training set and a testing set.
```{r, warning = FALSE, message = FALSE}
set.seed(9)
artists_split = initial_split(artists_model, prop = .3)
artists_train = training(artists_split)
artists_test = testing(artists_split)
```

Here is where we fit a classification tree on the training set. The structure of the code is as follows:

- First, we select the model type. In this case, we chose a decision tree.
- Then, we set the engine to **rpart** which is engine that produces decision trees.
- Next, we set the mode to **classification** because our target is categorical.
- Finally, we fit a model predicting `track_artist` based on the other features in the `artists_train` dataset. This is our trained model.
```{r, warning = FALSE, message = FALSE}
artists_tree = decision_tree() %>% 
  set_engine(engine = "rpart") %>% 
  set_mode(mode = "classification") %>% 
  fit(track_artist ~ ., data = artists_train)
```

The following code takes our trained model, `artists_tree`, and displays the split levels in our model.
```{r, warning = FALSE, message = FALSE}
rpart.plot(artists_tree$fit, roundint = FALSE, box.col = spotify_green, col = "black", branch.lwd = 3.5)
```

After displaying our classification tree, we generated predictions using the trained model to predict `track_artist` for the testing set and saved those predictions as `tree_pred`. We then created a new column called `pred_track_artist` by extracting the predictions from `tree_pred`. And lastly, we calculated the accuracy of our model's predictions.
```{r, warning = FALSE, message = FALSE}
tree_pred = artists_tree %>% 
  predict(new_data = artists_test)

artists_test %>% 
  mutate(pred_track_artist = tree_pred$.pred_class) %>% 
  accuracy(estimate = pred_track_artist, truth = track_artist)
```

Just to cover our bases, we decided to check for overfitting. To do this, we did the same steps as above, except this time we used the training set instead of the testing set. This is to make sure that the data isn't fit too well on the model it was trained on.
```{r, warning = FALSE, message = FALSE}
train_pred = artists_tree %>% 
  predict(new_data = artists_train)

artists_train %>% 
  mutate(pred_track_artist = train_pred$.pred_class) %>% 
  accuracy(estimate = pred_track_artist, truth = track_artist)
```

## Conclusion

Because our overall goal was to create a predictive model (a classification tree) predicting `track_artist` from a plethora of features, it is important to analyze the results to see if we were successful! We generated predictions on both the testing and training sets, along with the accuracy of the model for predicting `track_artist` for each set. The predictions made on the testing set had a 52.2% accuracy. Because our dataset contained 14 different artists, the model had 14 artists to 'choose from' when generating predictions. Given these constraints, we concluded that our model yielded decent results when predicting `track_artist`.

However, we did want to check for overfitting because our model had so many features. After generating predictions on the training set, which was the data the model was fit to, we calculated an accuracy of 75.9%. The model's predictions were 23.7% more accurate on the training set in comparison to the testing set. Because the difference in accuracies is on the higher side, it is worth noting that our model **may** be mildly overfit to the training set. One flaw that our model does have is the extreme specificity for the artist to predict. This model will only be useful in predicting `track_artist` for the 14 artists we have chosen, and no others outside of that list.

Throughout this project, we were able to exercise the extent of our knowledge on the data science process as a whole. We explored our data through different graphics, wrangled the data to create new datasets, and developed a predictive model.

